---
layout: post
title:  "Spark的总结"
crawlertitle: "关于spark的总结"
summary: "阅读OReilly.Learning.Spark.2015.1的总结"
date:   2018-03-15 13:07:00 +0700
cataname: 积累，等待，蜕变
category: tech/2018-03-15
tags: 'tech-大数据'
author: Roderick
---
`本文为阅读《OReilly.Learning.Spark.2015.1》这本书做出的一些简单总结。`


cluster manager：
有hadoop yarn，apache mesos， spark standalone
spark不依靠hadoop，hadoop的hdfs只是一个存储的系统。spark只是简单提供了操作hadoop的api，spark支持操作文本，SequenceFiles，Avro, Parquet, 和其他Hadoop InputFormat

spark的运行模式：  
本地：平常跑spark-shell，未配任何东西就是在local上跑作业的  
standalone：
spark on yarn：
spark on mesos：

写spark作业程序，需要初始化sparkcontext，可以通过制定master来指定是哪个模式。  
setMaster("local")比较特殊，只需要启动一个线程连接本地机器，不需要连接集群。  

创建RDD的两种方式：  
1.将已存在的数据集加载成为RDD  
2.在驱动程序上分发对象集（最简单的方式是使用SparkContext的parallelize()方法将collection的集合变换成RDD）  

RDD有两种类型的操作：  
1.transformations：从之前的RDD构造出新的RDD，返回类型是RDDs（filter()，map()）  
2.actions：在之前的RDD上操作，将其返回给驱动程序或者保存到外部存储系统，返回类型是其他数据类型（first()）  
这也是spark比hadoop的mr强大的一个地方，hadoop的mr还需要考虑如何优化将操作如何组合来减少数据传输  

（好像是说transformation）  
RDD使用懒加载，当定义一个RDD，从一个数据集加载进来的时候，实际上是还没做读操作，等到使用first()的时候，需要读到真正的数据时，RDD才会真正操作数据集，只读第一行  

RDD每次执行操作，都会重新计算RDD  
所以假如有一个RDD要做多次action，可以考虑把它先做持久化到内存/磁盘(RDD.persist()，好像.cache()方法也是可以的)  

有一个叫lineage graph的东西我也不太理解，好像是说spark会维护一个像图一样的数据来追踪两个RDD的transformation动作，用来当持久化的RDD已经失效，可以利用这些信息来做数据恢复。  

传递函数的时候需要注意，当传入一个对象的函数时，需要将函数赋值给另一个本地变量，再将本地变量传入。因为不这么做，在传递参数的时候，会把整个对象都给传递过去，浪费流量和带宽。  

flatmap和map的区别，map是将一个iterator每个元素都映射成一个新元素，新元素组成新的RDD集合，flatmap是将iterator的每个元素“压扁”，其实换句话说就是将每个元素都拆解出来成多个新的元素，然后这些新的元素组合成一个新的RDD集合。  
例如：  
{"A B","C D","E F"} 做了map(x=> x.split)之后是{["A","B"],["C","D"],["E","F"]}  
{"A B","C D","E F"} 做了flatmap(x=> x.split)之后是{"A","B","C","D","E","F"}  
最明显的区别就是map之后元素的数量是不变的，flatmap可能会变多。

通用的RDD transformation函数：map(), flatMap(), fliter(), distinct(), union(), intersection(), subtract()，cartesian()，sample()  
intersection() 和 subtract() 方法会引起shuffle  
cartesian() 笛卡尔积将两个RDD每个元素的组合(a,b)变成一个新的RDD（不过对于大数据RDD会有点耗资源）  

通用的RDD action函数：reduce(), fold(), aggergate(), collect(), take(n), top(), takeSample(), count(), countByValue()，foreach()  
aggergate不限制输出类型和输入类型一致。  
take(n) 返回的数据是无序的  
fold和reduce差不多，只是多了个初始化参数，假如参数为0则结果一致.  

持久化有多个级别：MEMORY_ONLY, MEMORY_ONLY_SER, MEMORY_AND_DISK, MEMORY_AND_DISK_SER, DISK_ONLY
