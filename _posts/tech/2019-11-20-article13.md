---
layout: post
title:  "kafka集群依赖的zookeeper迁移"
crawlertitle: "kafka的初体验"
summary: "将kafka集群的zk迁移至一个全新的zk集群，并且保证kafka的正常使用"
date:   2019-11-20 23:50:00 +0700
cataname: 积累，等待，蜕变
category: tech/2019-11-20
tags: 'tech-消息队列'
author: Roderick
---
`由于公司的kafka特殊原因需要将原先的zk集群迁移至另外一个zk集群`

首先要确定一下整个迁移方案（注意，这里是允许有少量数据丢失的）  
1. 关闭整个kafka集群
2. 修改每个kafka机器的配置，将zk配置改成新的集群地址
3. 将旧的zk集群数据拷贝到新的zk集群
4. 重启整个kafka集群

**注意事项**

1. 直接依次kill/关掉 kafka会不会造成什么后果（比如保存了什么信息到了zk）重启的时候会不会有什么影响
2. 在关闭kafka的整个过程，kafka还能否正常生产和消费。
3. 迁移之后，kafka的topic的每个分区的leader能否自动均衡的分布，isr是否正常。


**本地测试**

本地测试采用的是docker三个容器，部署三个kafka，宿主机和其中一台容器作为zk服务器。  
1. 配置每个kafka的broker.id为0,1,2，zookeeper配置为宿主机ip  
2. 创建topic  
{% highlight shell %}  
$./bin/kafka-topics.sh --zookeeper 172.18.0.2:2181/kafka --create --replication-factor 3 --partitions 3 --topic test_topic4  
{% endhighlight %}  
3. 查看topic的状态：  
{% highlight shell %}  
$./bin/kafka-topics.sh --zookeeper 172.18.0.2:2181/kafka --describe -topic test_topic4
Topic:test_topic4       PartitionCount:3        ReplicationFactor:3     Configs:
        Topic: test_topic4      Partition: 0    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2
        Topic: test_topic4      Partition: 1    Leader: 1       Replicas: 1,2,0 Isr: 1,2,0
        Topic: test_topic4      Partition: 2    Leader: 2       Replicas: 2,0,1 Isr: 2,0,1
{% endhighlight %}  
4. 逐步关闭kafka，先关闭broker0，测试消费和生产是ok的，
{% highlight shell %}  
$./bin/kafka-console-producer.sh  --broker-list 172.18.0.3:9092,172.18.0.2:9092,172.18.0.4:9092 --topic test_topic5
heheh

$./bin/kafka-console-consumer.sh --bootstrap-server 172.18.0.3:9092,172.18.0.2:9092,172.18.0.4:9092 --from-beginning --topic test_topic5
heheh
{% endhighlight %}  
再查看topic状态：  
{% highlight shell %}  
$./bin/kafka-topics.sh --zookeeper 172.18.0.2:2181/kafka --describe -topic test_topic4
Topic:test_topic4       PartitionCount:3        ReplicationFactor:3     Configs:  
        Topic: test_topic4      Partition: 0    Leader: 1       Replicas: 0,1,2 Isr: 1,2  
        Topic: test_topic4      Partition: 1    Leader: 1       Replicas: 1,2,0 Isr: 1,2  
        Topic: test_topic4      Partition: 2    Leader: 2       Replicas: 2,0,1 Isr: 2,1  
{% endhighlight %}  
再关闭broker1，测试消费和生产是ok的，再查看topic状态：
{% highlight shell %}  
$./bin/kafka-topics.sh --zookeeper 172.18.0.2:2181/kafka --describe -topic test_topic4
Topic:test_topic4       PartitionCount:3        ReplicationFactor:3     Configs:  
        Topic: test_topic4      Partition: 0    Leader: 2       Replicas: 0,1,2 Isr: 2  
        Topic: test_topic4      Partition: 1    Leader: 2       Replicas: 1,2,0 Isr: 2  
        Topic: test_topic4      Partition: 2    Leader: 2       Replicas: 2,0,1 Isr: 2  
{% endhighlight %}  
然后再关闭最后一台，此时查到的topic和上一次还是一致，我估计是要隔一段时间才会将leader改为-1  
5. 将每台kafka的zk地址改为那台安装了zk的容器。  
zookeeper.connect=192.168.244.133:2181  
6. 使用zkcopy迁移zk的数据（上github搜，需要maven编译）  
{% highlight shell %}  
java -jar target/zkcopy.jar --source 172.18.0.2:2181/kafka --target 192.168.244.133:2181/kafka
{% endhighlight %}  
7. 依次重启每台kafka机器，并且查看topic的状态和消费生产功能。  
会发现消费和生产能力是ok的，isr逐步会将重启的broker加入，最后isr和副本正常，但是leader依然是只在一台机上。这是由于我的kafka没有开启auto.leader.rebalance.enable=true，自动负载均衡，所以此时可以通过手动来均衡负载
{% highlight shell %}  
./bin/kafka-preferred-replica-election.sh --zookeeper 192.168.244.133:2181/kafka      
{% endhighlight %}  
8. 发现此时topic的状态已经恢复成迁移前的状态了。

**踩坑**

发生了好几个乌龙。  
1.kafka用了不同网段的容器，容器之间ping不通，导致broker启动的时候会报连接失败的错误   
2.容器ping的通zk集群，但是zk集群ping不通容器，导致我恢复kafka的时候，虽然zk的/kafka/broker/ids里已经更新了，但是查看topic的isr还是没有更新。  
3.kafka版本不一致导致kafka重启报了kafka start invalid version for api key的异常  
