---
layout: post
title:  "structured streaming的入门demo"
crawlertitle: "ss的入门"
summary: "ss属于比较新的东西，文档相对比较少。只能摸索摸索了"
date:   2019-04-24 13:10:00 +0700
cataname: 积累，等待，蜕变
category: tech/2019-04-24
tags: 'tech-大数据'
author: Roderick
---
`看了官网文档（http://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html）后稍微做的一点记录`

我简单过了一下，比较有用的东西应该是窗体函数了，所以这里简单列举了一下简单操作和窗体函数的用法
**简单操作**

翻译官网的一个监听socket流的demo

1.创建spark session
{% highlight java %}
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.sql.*;
import org.apache.spark.sql.streaming.StreamingQuery;

import java.util.Arrays;
import java.util.Iterator;

SparkSession spark = SparkSession
  .builder()
  .appName("JavaStructuredNetworkWordCount")
  .getOrCreate();
{% endhighlight %}

2.创建dataframe和dataset
{% highlight java %}
// Create DataFrame representing the stream of input lines from connection to localhost:9999
// -> 相当于一个unbounded table
Dataset<Row> lines = spark
  .readStream()
  .format("socket")
  .option("host", "localhost")
  .option("port", 9999)
  .load();


// Split the lines into words
Dataset<String> words = lines
  .as(Encoders.STRING())
  .flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());


// Generate running word count  -> 相当于一个result
Dataset<Row> wordCounts = words.groupBy("value").count();
{% endhighlight %}

3.开始执行查询流数据
{% highlight java %}
// Start running the query that prints the running counts to the console
StreamingQuery query = wordCounts.writeStream()
  .outputMode("complete")
  .format("console")
  .start();


query.awaitTermination();
{% endhighlight %}

**窗体函数**
设置窗口时长为10分钟，滑动时长为5分钟，效果如图：
切割成如下的窗体
12:00 - 12:10
12:05 - 12:15
12:10 - 12:20
12:15 - 12:25
到来的事件落到哪个区间，就会在哪个区间的数据上累加计算，最终随着时间流水线，12:15分的时候可以看到三个区间里的统计数据。
[![attache1]({{ site.article_attachedimages | relative_url }}/tech/article12/attache1.png)]({{ site.article_attachedimages | relative_url }}/tech/article12/attache1.png)

处理延迟数据： watermarking
当事件到来有延迟，刚好错过了时间区间，有一个补救措施，设置一个watermark，假如设置为10分钟，12:04分的数据等到12:11分才到来，当前最大的事件时间是12:13，所以12:13-10分钟之前的数据会被丢弃，也就是12:03之后的数据可以被更新，所以这个事件可以被算进之前的区间里，数据刚好落在12:00 - 12:05，所以这部分数据会被更新，12:15输出的数据就可以看到对应的更新纠正。
[![attache2]({{ site.article_attachedimages | relative_url }}/tech/article12/attache2.png)]({{ site.article_attachedimages | relative_url }}/tech/article12/attache2.png)

