---
layout: post
title:  "编写java版本的spark程序"
crawlertitle: "记录一些细节"
summary: "最近想拿一个简单的任务来实践一下spark，在运行的时候就遇到一些问题，记录一下"
date:   2019-12-08 23:50:00 +0700
cataname: 积累，等待，蜕变
category: tech/2019-12-08
tags: 'tech-大数据'
author: Roderick
---
`在写spark的demo程序遇到的一些小问题`

# 搭建项目
这里使用maven作为构建和依赖管理工具，开发平台用idea。首先创建一个maven的java项目，这个比较简单就不细说了。

# 编写maven依赖
首先spark用的版本是2.2.0，scala版本是2.11.0，所以在选择maven依赖的时候需要注意版本号，比如说：  
spark-core的maven配置为：
{% highlight xml lineanchors %}
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.2.0</version>
</dependency>
{% endhighlight %}  
artifactid里spark-core_后面的版本号是代表用具体版本号的scala编译，version才是表示spark的版本号  
接着我继续添加spark-sql的依赖，结果就是：
{% highlight xml lineanchors %}
<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.2.0</version>
</dependency>

<!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>2.2.0</version>
</dependency>
{% endhighlight %}  
然后发现idea下载特别的慢，所以修改了maven的镜像配置为阿里的云仓库：
{% highlight xml lineanchors %}
<mirror>
  <id>main</id>
  <mirrorOf>foo</mirrorOf>
  <name>Local Repository</name>
  <url>http://repository.polyv.net/nexus/content/groups/public/</url>
</mirror>
{% endhighlight %}  
接着idea需要配置使用~/.m2/setting.xml:  
[![attache1]({{ site.article_attachedimages | relative_url }}/tech/20191208_article1/attache1.png)]({{ site.article_attachedimages | relative_url }}/20191208_article1/attache1.png)  
燃鹅，我配置完了之后，发现下载依赖还是用了中央仓库的源。查了资料看到有人这么写：
{% highlight xml lineanchors %}
<repositories>
    <repository>
        <id>nexus-aliyun</id>
        <name>nexus-aliyun</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
            <enabled>true</enabled>
        </releases>
        <snapshots>
            <enabled>false</enabled>
        </snapshots>
    </repository>
</repositories>
{% endhighlight %}   
嗯，真香。好下面进入正题，也是踩坑的开始  
当我准备写一个简单的spark程序：
{% highlight java %}
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class Test {

    public static void main(String[] args) {

        SparkSession spark = SparkSession
                .builder()
                .master("local")
                .appName("count for nginx")
                .getOrCreate();
        Dataset<Row> df = spark.read().json("input/test.json");

        df.show();
    }
}
{% endhighlight %}  
写完发现org.apache.spark.sql.Row 根本就找不到，莫非我打开方式不对，查了资料，看到 https://stackoverflow.com/questions/37438673/org-apache-spark-sql-row-cannot-be-resolved-in-spark-2-0-preview
 有人也是遇到此问题，有人评论spark-catalyst_2.11可以解决问题，我去查maven的配置，加了这么一段进去：
{% highlight xml lineanchors %}
 <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-catalyst_2.11</artifactId>
    <version>2.2.0</version>
    <scope>test</scope>
</dependency>
{% endhighlight %}  
但是没起作用，查了很多资料都未果，再最后，去翻了之前写的程序，看到org.apache.spark.sql.Row确实是来源于spark-catalyst的依赖的。我再回过头看我写的maven依赖  
我擦。。scope写错了，应该是compile。。。立马改回去，重新构建，成功了。。  
